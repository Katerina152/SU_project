The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) devel   2) math
/scratch/users/ksa828/venvs/cool_project/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[rank: 0] Seed set to 42
2025-12-13 15:02:09,195 | INFO | [extract] Logging to ISIC2017/isic_dinov3_emb_512/seed_42/extract_embeddings.log
2025-12-13 15:02:09,195 | INFO | [extract] Experiment root = ISIC2017/isic_dinov3_emb_512
2025-12-13 15:02:09,196 | INFO | [extract] Seed directory  = ISIC2017/isic_dinov3_emb_512/seed_42
2025-12-13 15:02:09,196 | INFO | [dermatology] Train transform:
Compose(
    Resize(size=(512, 512), interpolation=bicubic, max_size=None, antialias=True)
    ToTensor()
    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
)
2025-12-13 15:02:09,196 | INFO | [dermatology] Eval  transform:
Compose(
    Resize(size=(512, 512), interpolation=bicubic, max_size=None, antialias=True)
    ToTensor()
    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
)
2025-12-13 15:02:09,196 | INFO | [dermatology] Test  transform:
Compose(
    Resize(size=(512, 512), interpolation=bicubic, max_size=None, antialias=True)
    ToTensor()
    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
)
/scratch/users/ksa828/venvs/cool_project/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
2025-12-13 15:02:09,463 | INFO | [extract] Dataloaders created with keys: ['train', 'val', 'test']
You are using a model of type dinov3_vit to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.
Some weights of ViTModel were not initialized from the model checkpoint at facebook/dinov3-vits16-pretrain-lvd1689m and are newly initialized: ['embeddings.patch_embeddings.projection.bias', 'embeddings.patch_embeddings.projection.weight', 'embeddings.position_embeddings', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'layernorm.bias', 'layernorm.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-12-13 15:02:10,309 | INFO | [extract] Model built: VisionTransformerWithHead
2025-12-13 15:02:10,329 | INFO | ===== Trainer hardware configuration (embeddings) =====
2025-12-13 15:02:10,329 | INFO | SLURM num_nodes          = 1
2025-12-13 15:02:10,329 | INFO | CUDA visible GPUs        = 1
2025-12-13 15:02:10,329 | INFO | use_gpu                  = True
2025-12-13 15:02:10,329 | INFO | Chosen Lightning strategy= auto
2025-12-13 15:02:10,329 | INFO | ======================================================
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
2025-12-13 15:02:10,357 | INFO | [extract] Embeddings will be saved under: ISIC2017/isic_dinov3_emb_512/seed_42/embeddings
2025-12-13 15:02:10,357 | INFO | [extract] Extracting embeddings for split: train
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
/scratch/users/ksa828/venvs/cool_project/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:485: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
2025-12-13 15:04:58,847 | INFO | [extract] Saved train embeddings to ISIC2017/isic_dinov3_emb_512/seed_42/embeddings/train_embeddings.pt, embeddings shape=torch.Size([2000, 384]), labels shape=torch.Size([2000]), ids=2000
2025-12-13 15:04:58,849 | INFO | [extract] Extracting embeddings for split: val
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
2025-12-13 15:05:25,901 | INFO | [extract] Saved val embeddings to ISIC2017/isic_dinov3_emb_512/seed_42/embeddings/val_embeddings.pt, embeddings shape=torch.Size([150, 384]), labels shape=torch.Size([150]), ids=150
2025-12-13 15:05:25,902 | INFO | [extract] Extracting embeddings for split: test
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
2025-12-13 15:07:34,092 | INFO | [extract] Saved test embeddings to ISIC2017/isic_dinov3_emb_512/seed_42/embeddings/test_embeddings.pt, embeddings shape=torch.Size([600, 384]), labels shape=torch.Size([600]), ids=600
2025-12-13 15:07:34,098 | INFO | [extract] Wrote metadata to ISIC2017/isic_dinov3_emb_512/seed_42/embeddings/metadata.json
2025-12-13 15:07:34,098 | INFO | [extract] Finished embedding extraction.
