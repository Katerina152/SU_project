The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) devel   2) math
/scratch/users/ksa828/venvs/cool_project/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[rank: 0] Seed set to 42
2025-12-14 22:55:37,614 | INFO | [distill] Logging to runs/ISIC2017/isic_distillation_vit_binary_224/seed_42/train_distill.log
2025-12-14 22:55:37,614 | INFO | [distill] Domain=dermatology dataset_name=ISIC2017
2025-12-14 22:55:37,614 | INFO | [distill] seed_dir=runs/ISIC2017/isic_distillation_vit_binary_224/seed_42
2025-12-14 22:55:37,614 | INFO | [dermatology] Train transform:
Compose(
    Resize(size=(224, 224), interpolation=bicubic, max_size=None, antialias=True)
    ToTensor()
    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
)
2025-12-14 22:55:37,614 | INFO | [dermatology] Eval  transform:
Compose(
    Resize(size=(224, 224), interpolation=bicubic, max_size=None, antialias=True)
    ToTensor()
    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
)
2025-12-14 22:55:37,614 | INFO | [dermatology] Test  transform:
Compose(
    Resize(size=(224, 224), interpolation=bicubic, max_size=None, antialias=True)
    ToTensor()
    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
)
/scratch/users/ksa828/venvs/cool_project/lib/python3.12/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
2025-12-14 22:55:37,760 | INFO | [distill] Loaded teacher map for split='train': N=2000, D=384
2025-12-14 22:55:37,764 | INFO | [distill] Loaded teacher map for split='val': N=150, D=384
2025-12-14 22:55:37,770 | INFO | [distill] Loaded teacher map for split='test': N=600, D=384
2025-12-14 22:55:47,829 | INFO | [distill] First batch x.shape=torch.Size([32, 3, 224, 224]) teacher.shape=torch.Size([32, 384])
2025-12-14 22:55:48,167 | INFO | Loading pretrained weights from Hugging Face hub (timm/tiny_vit_21m_224.in1k)
2025-12-14 22:55:48,338 | INFO | [timm/tiny_vit_21m_224.in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
2025-12-14 22:55:48,446 | INFO | [distill] Starting distillation training...
/scratch/users/ksa828/venvs/cool_project/lib/python3.12/site-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory runs/ISIC2017/isic_distillation_vit_binary_224/seed_42/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name         | Type                      | Params | Mode  | FLOPs
---------------------------------------------------------------------------
0 | model        | VisionTransformerWithHead | 20.6 M | train | 0    
1 | proj         | Linear                    | 221 K  | train | 0    
2 | distill_loss | EmbeddingDistillationLoss | 0      | train | 0    
---------------------------------------------------------------------------
20.8 M    Trainable params
0         Non-trainable params
20.8 M    Total params
83.373    Total estimated model params size (MB)
266       Modules in train mode
0         Modules in eval mode
0         Total Flops
SLURM auto-requeueing enabled. Setting signal handlers.
`Trainer.fit` stopped: `max_epochs=30` reached.
2025-12-14 23:58:31,793 | INFO | [distill] Starting test...
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
2025-12-14 23:59:57,004 | INFO | [distill] Done.
